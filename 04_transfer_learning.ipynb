{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyONua9o+Y4Db2Bu8pllr6wx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divyalaldinani/Deep-Learning-in-TensorFlow/blob/main/04_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transfer learning:\n",
        "leveraging a working model's existing architecture and learned patterns for our own problem\n",
        "\n",
        "> use/make use of existing trained model proven to work on a problem similar to ours.\n",
        "<br>\n",
        "> learned patterns on similar data to our own, then we can adapt those patterns to our own dataset."
      ],
      "metadata": {
        "id": "cN9O4PSStMee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow==2.14.0 tensorflow-hub==0.15.0 keras==2.14.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Z16MFS0pKeXS",
        "outputId": "ef0c9cb3-714c-42dd-e1a3-7ab5fce2c6b9"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.14.0\n",
            "  Downloading tensorflow-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting tensorflow-hub==0.15.0\n",
            "  Downloading tensorflow_hub-0.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras==2.14.0\n",
            "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.68.1)\n",
            "Collecting tensorboard<2.15,>=2.14 (from tensorflow==2.14.0)\n",
            "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow==2.14.0)\n",
            "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.14.0) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.27.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow==2.14.0)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.2.2)\n",
            "Downloading tensorflow-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.8/489.8 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_hub-0.15.0-py2.py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: tensorflow-hub, tensorflow-estimator, keras, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-hub\n",
            "    Found existing installation: tensorflow-hub 0.12.0\n",
            "    Uninstalling tensorflow-hub-0.12.0:\n",
            "      Successfully uninstalled tensorflow-hub-0.12.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.15.0\n",
            "    Uninstalling tensorflow-estimator-2.15.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.15.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.14.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-1.0.0 keras-2.14.0 tensorboard-2.14.1 tensorflow-2.14.0 tensorflow-estimator-2.14.0 tensorflow-hub-0.15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google_auth_oauthlib",
                  "keras",
                  "tensorflow",
                  "tensorflow_hub"
                ]
              },
              "id": "a60a9784a93842d6af210319ad650834"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#are we using a GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8zWo6uStwp1",
        "outputId": "178d46de-1fe2-452e-da2f-c9ff84ece90f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jan  6 15:45:15 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading and visualizing data -> https://www.kaggle.com/datasets/dansbecker/food-101 -> 10 categories -> each category has 10% of org data as present in actual\n",
        "\n",
        "import zipfile\n",
        "\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
        "\n",
        "zip_ref = zipfile.ZipFile('10_food_classes_10_percent.zip')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56DtpsHmuC_N",
        "outputId": "b8da6f35-eb2e-428e-c5a8-a6b2333cbc29"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-06 15:45:17--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.107.207, 108.177.98.207, 74.125.197.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.107.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 168546183 (161M) [application/zip]\n",
            "Saving to: ‘10_food_classes_10_percent.zip.11’\n",
            "\n",
            "10_food_classes_10_ 100%[===================>] 160.74M   239MB/s    in 0.7s    \n",
            "\n",
            "2025-01-06 15:45:18 (239 MB/s) - ‘10_food_classes_10_percent.zip.11’ saved [168546183/168546183]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inspecting the data\n",
        "\n",
        "import os\n",
        "\n",
        "for dirpath, dirnames, filenames in os.walk('10_food_classes_10_percent'):\n",
        "    print(f\"{len(dirnames)} dirs and {len(filenames)} files are present in {dirpath}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVKZ53mYuW9c",
        "outputId": "ef4c9edb-7b5b-478e-abb3-9ccbf71e8125"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 dirs and 0 files are present in 10_food_classes_10_percent\n",
            "10 dirs and 0 files are present in 10_food_classes_10_percent/train\n",
            "0 dirs and 75 files are present in 10_food_classes_10_percent/train/ice_cream\n",
            "0 dirs and 75 files are present in 10_food_classes_10_percent/train/chicken_wings\n",
            "0 dirs and 75 files are present in 10_food_classes_10_percent/train/chicken_curry\n",
            "0 dirs and 75 files are present in 10_food_classes_10_percent/train/fried_rice\n",
            "0 dirs and 75 files are present in 10_food_classes_10_percent/train/ramen\n",
            "0 dirs and 75 files are present in 10_food_classes_10_percent/train/hamburger\n",
            "0 dirs and 75 files are present in 10_food_classes_10_percent/train/steak\n",
            "0 dirs and 75 files are present in 10_food_classes_10_percent/train/pizza\n",
            "0 dirs and 75 files are present in 10_food_classes_10_percent/train/sushi\n",
            "0 dirs and 75 files are present in 10_food_classes_10_percent/train/grilled_salmon\n",
            "10 dirs and 0 files are present in 10_food_classes_10_percent/test\n",
            "0 dirs and 250 files are present in 10_food_classes_10_percent/test/ice_cream\n",
            "0 dirs and 250 files are present in 10_food_classes_10_percent/test/chicken_wings\n",
            "0 dirs and 250 files are present in 10_food_classes_10_percent/test/chicken_curry\n",
            "0 dirs and 250 files are present in 10_food_classes_10_percent/test/fried_rice\n",
            "0 dirs and 250 files are present in 10_food_classes_10_percent/test/ramen\n",
            "0 dirs and 250 files are present in 10_food_classes_10_percent/test/hamburger\n",
            "0 dirs and 250 files are present in 10_food_classes_10_percent/test/steak\n",
            "0 dirs and 250 files are present in 10_food_classes_10_percent/test/pizza\n",
            "0 dirs and 250 files are present in 10_food_classes_10_percent/test/sushi\n",
            "0 dirs and 250 files are present in 10_food_classes_10_percent/test/grilled_salmon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test data is same but train data is 10% of original"
      ],
      "metadata": {
        "id": "qSPaCEG_vXMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Creating data loaders*\n",
        "**using ```ImageDataGenerator``` to load in our images in batches**\n",
        "italicized text\n",
        "\n"
      ],
      "metadata": {
        "id": "KF-mUxRmv73U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "IMAGE_SHAPE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dir = '10_food_classes_10_percent/train/'\n",
        "test_dir = '10_food_classes_10_percent/test/'\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(train_dir,\n",
        "                                               target_size=IMAGE_SHAPE,\n",
        "                                               batch_size=BATCH_SIZE,\n",
        "                                               class_mode='categorical')\n",
        "\n",
        "test_data = test_datagen.flow_from_directory(test_dir,\n",
        "                                             target_size=IMAGE_SHAPE,\n",
        "                                             batch_size= BATCH_SIZE,\n",
        "                                             class_mode = 'categorical')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OC7p-m9cwxV6",
        "outputId": "07f42024-cb53-4368-84e9-c86b4ac399bb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 750 images belonging to 10 classes.\n",
            "Found 2500 images belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Setting up callbacks(things to run during our model trains)*\n",
        "\n",
        "Callbacks are extra functionality to be executed during or after model training.\n",
        "Popular callbacks:\n",
        "* Tracking experiments with TensorBoard callback\n",
        "* Model checkpoint with ModelCheckpoint callback\n",
        "* Stopping a model from training( as no progress in accuracy) with EarlyStopping Callback"
      ],
      "metadata": {
        "id": "yLrY19_BzNXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from re import template\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "\n",
        "def create_tb_callback(dir_name, exp_name):\n",
        "    log_dir = dir_name + '/' + exp_name + '/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir)\n",
        "    print(f\"Saving tensorboard log files to: {log_dir}\")\n",
        "    return tensorboard_callback"
      ],
      "metadata": {
        "id": "XeZ34pQezYgm"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also save TensorBoard log( model training metrics) in the log_dir directory."
      ],
      "metadata": {
        "id": "hcFibwNX1_7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating models using TensorFlow Hub\n",
        "\n",
        "Accessing pretrained models fron TensorFlow Hub.\n",
        "[here](https://tfhub.dev)"
      ],
      "metadata": {
        "id": "3OPNa39D2Ljq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# efficientnet_url = \"https://www.kaggle.com/models/google/efficientnet-v2/TensorFlow2/imagenet1k-b0-feature-vector/2\"\n",
        "\n",
        "efficientnet_url = \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\""
      ],
      "metadata": {
        "id": "NDzpWJnS1qYQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tf_keras\n",
        "\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "oJo1Ii-P2PnT"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install tensorflow==2.15.0 tensorflow-hub keras==2.15.0"
      ],
      "metadata": {
        "id": "NR87i2VAL6Wg"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(model_url):\n",
        "  \"\"\"\n",
        "  doc string, wohhoo!!\n",
        "  \"\"\"\n",
        "  model = tf.keras.Sequential([\n",
        "      hub.KerasLayer(model_url,\n",
        "                    trainable=False, # freeze the underlying patterns\n",
        "                    name='feature_extraction_layer',\n",
        "                    input_shape=IMAGE_SHAPE+(3,)), # define the input image shape\n",
        "\n",
        "    layers.Dense(10, activation='softmax') # create our own output layer\n",
        "  ])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "_Xp_Tulp2PCM"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "efficientnet_model = create_model(efficientnet_url)\n",
        "\n",
        "efficientnet_model.compile(loss = 'categorical_crossentropy',\n",
        "                           optimizer = tf.keras.optimizers.Adam(),\n",
        "                           metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Z1RL-8N4FhId"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lidat"
      ],
      "metadata": {
        "id": "Zr0d4i22T-50"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "efficientnet_history = efficientnet_model.fit(train_data,\n",
        "                                              validation_data=test_data,\n",
        "                                              batch_size=BATCH_SIZE,\n",
        "                                              epochs=10,\n",
        "                                              callbacks=[create_tb_callback(dir_name=\"tensorflow_hub\", exp_name=\"efficientnetb0-1\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHIrPYE6Q_87",
        "outputId": "587f9456-10cc-48b1-8b58-ac029c581e4c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving tensorboard log files to: tensorflow_hub/efficientnetb0-1/20250106-155407\n",
            "Epoch 1/10\n",
            "24/24 [==============================] - 193s 8s/step - loss: 1.8449 - accuracy: 0.4307 - val_loss: 1.3027 - val_accuracy: 0.7148\n",
            "Epoch 2/10\n",
            "24/24 [==============================] - 180s 8s/step - loss: 1.0802 - accuracy: 0.7520 - val_loss: 0.8712 - val_accuracy: 0.8120\n",
            "Epoch 3/10\n",
            "24/24 [==============================] - 179s 8s/step - loss: 0.7690 - accuracy: 0.8293 - val_loss: 0.7005 - val_accuracy: 0.8376\n",
            "Epoch 4/10\n",
            "24/24 [==============================] - 181s 8s/step - loss: 0.6213 - accuracy: 0.8667 - val_loss: 0.6122 - val_accuracy: 0.8544\n",
            "Epoch 5/10\n",
            "24/24 [==============================] - 178s 8s/step - loss: 0.5216 - accuracy: 0.8907 - val_loss: 0.5576 - val_accuracy: 0.8600\n",
            "Epoch 6/10\n",
            "24/24 [==============================] - 180s 8s/step - loss: 0.4501 - accuracy: 0.9107 - val_loss: 0.5218 - val_accuracy: 0.8608\n",
            "Epoch 7/10\n",
            "24/24 [==============================] - 179s 8s/step - loss: 0.3953 - accuracy: 0.9200 - val_loss: 0.4944 - val_accuracy: 0.8708\n",
            "Epoch 8/10\n",
            "24/24 [==============================] - 178s 8s/step - loss: 0.3532 - accuracy: 0.9240 - val_loss: 0.4730 - val_accuracy: 0.8724\n",
            "Epoch 9/10\n",
            "24/24 [==============================] - 183s 8s/step - loss: 0.3166 - accuracy: 0.9467 - val_loss: 0.4592 - val_accuracy: 0.8716\n",
            "Epoch 10/10\n",
            "24/24 [==============================] - 174s 8s/step - loss: 0.2859 - accuracy: 0.9587 - val_loss: 0.4426 - val_accuracy: 0.8756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpY_Xm9UF9Ik",
        "outputId": "174a480f-84a9-42ca-a9a7-c1a8c2c39c77"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.14.0\n",
            "0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.image as mpimg\n",
        "\n",
        "img = mpimg.imread('/content/10_food_classes_10_percent/test/ice_cream/1272987.jpg')\n",
        "# efficientnet_model.predict(img)\n",
        "img = tf.image.resize(img, (224, 224))\n",
        "img = tf.convert_to_tensor(img)\n",
        "img = tf.expand_dims(img, axis=0)\n",
        "img.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1i87CqcOMImJ",
        "outputId": "8a3c29f9-b615-4773-e1b5-7ed6d4071100"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 224, 224, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install tensorflow tensorflow-hub --upgrade\n",
        "img /= 255.\n",
        "efficientnet_model.predict(img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drlI3LmgIAHP",
        "outputId": "c6c1aba8-3f1c-465e-961a-79f2d36fe473"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 817ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.03986428, 0.0229838 , 0.41536084, 0.00371772, 0.02489673,\n",
              "        0.4532505 , 0.0059753 , 0.00319135, 0.00343768, 0.02732182]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = os.listdir(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "z0bYxXfUIC4V",
        "outputId": "d4782615-08ce-4e31-bb02-d0990d6938fa"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "listdir: path should be string, bytes, os.PathLike, integer or None, not DirectoryIterator",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-3c8251850efe>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: listdir: path should be string, bytes, os.PathLike, integer or None, not DirectoryIterator"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aWOzUk1refkN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}